# MonocularDepthEstimation

This repository contains the Tensorflow implementation for a Monocular Depth Estimator.

## Introduction:

Depth Estimation is one of the fundamental and most challenging task in Computer Vision; that has a wide range of applications like scene understanding, scene reconstruction(2D to 3D), virtual and augumented reality, robotics, autonomous vehicles and many more.

In general there are two ways in which this task is achieved:-
  1. Multi-source method.
  2. Single-source or Monocular-source method.

### Multi-source method:
Most of the work related to depth estimation has been traditionally done by taking inputs of the same scene from multiple sources placed within slight or minor displacement between them and then matching keypoints that are common within each image to create a 3 dimensional spatial representation of the scene. This method is analogous to how our brain estimates the depth of the spatial structures surrounding us by processing the inputs from both the eyes whose viewpoints are at a minor displacement from each other; this is also known as Binocular stereopsis or Stereo Vision. A popular real-world example of using stereo vision to create an effect of depth are the dual camera technology or dual pixel technology (popularly used in Google's Pixel 2 to create depth effects from a single camera, but using the displacement generated by individual pixel seperation). The Scale-Invariant Feature Transform or the SIFT algorithm works really well for this task.
Also some technologies use the time of flight principle to calculate the distance of an object from the source where one source projects a particle or wave through the environment like infrared waves in the case of Kinect or Laser pulses in the case of LIDAR systems and the other source calculates the time it took for the projectile to bounce-off or get reflected back from the objects in the environment to create a spatial representation of the surrounding.
These approaches produce high defination depth maps but are often expensive to operate and do affect portability.

### Single/Monocular-source method:
With the incredible amount of research and development in the fields of deep learning and its integration with computer vision, we have the ability to produce high quality depth maps using a monocular source that is generally inexpensive and more portable.
Some of the popular learning models to train the deep neural network architecture for this task are as follows:-
  1. Supervised Learning Models:-
  The supervised learning model takes advantage of a labelled dataset, where each input image is accompanied by depth map for the model to train upon and infer a rich feature palette in order to generalize it on an image taken from a single source camera. Some popular datasets used for this task are NYUv2 dataset, KITTI dataset and the MAKE3D dataset. An encoder-decoder style neural network has proven to be a good architecture to extract the depth information from the inputs.
  
  2. Semi-supervised Models:-
  Since it is hard to obtain high quality depth datasets that account for all the possible environments, it provides hinderance in improving the performance of a supervised learning model beyond a certain level. To counter this problem newer semi-supervised or self-supervised models were introduced ( like the one used by Tesla in their self-driving cars) which work by training the neural network to generate a right-sided image consistent with its left-sided counterpart or vice-versa and then calculate the disparity between them and use it along with the focal length of the camera to calculate the depth of the surrounding objects.
  
## Project Description:

This project uses a supervised learning approach to train an encoder-decoder style neural network architecture where the encoding path uses transfer learning to extract rich features from a pre-trained DenseNet169 model to produce high-resolution depth maps with the help of the decoding path. The dataset used for this task is the [NYUv2 Depth Dataset 50K images](https://s3-eu-west-1.amazonaws.com/densedepth/nyu_data2.zip).

## Dependencies:
* Python ==> 3.7.6
* tensorflow ==> 2.2.0
* numpy ==> 1.18.1
* matplotlib ==> 3.1.3
* opencv ==> 4.2.0.34
* scikit-image ==> 0.16.2

## Usage:
1. Make sure the above dependencies are satisfied and all the files associated with this project are in the same folder.

2. To train the model simply open the python notebook DenseDepth.ipynb and run it entirely.

3. To test the model on any image simply open the python notebook DepthTest.ipynb and run it entirely while specifying the image path to get the output.

## Results:

#### Input Image:
![alt text](https://github.com/yashdubey95/MonocularDepthEstimation/blob/master/images/Input.PNG)

### Output Image (cmap = Plasma):
![alt text](https://github.com/yashdubey95/MonocularDepthEstimation/blob/master/images/Ouput.PNG)

### Input-Output Montage:
![alt text](https://github.com/yashdubey95/MonocularDepthEstimation/blob/master/images/input_output_montage.PNG)


